{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6565f2be-dbe3-441a-bede-87c5c7baedcd",
   "metadata": {},
   "source": [
    "# $$ Step\\ 5\\ : Tokenization\\ $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92362fe8-aadf-46cd-a4fe-5bba09e50615",
   "metadata": {},
   "source": [
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a71a39-aa30-42f8-a36c-952d6135e2d9",
   "metadata": {},
   "source": [
    "### Tokenizing Text  \n",
    "\n",
    "A key step in Natural Language Processing (NLP) is **tokenization**, which involves breaking down text into smaller components called **tokens**. The most common type is **word tokenization**, where each word in a sentence is treated as an individual token. However, tokenization can also occur at different levels, such as **sentence tokenization**, **subword tokenization**, or even **character tokenization**, depending on the application.  \n",
    "\n",
    "### Why is Tokenization Important?  \n",
    "Tokenization helps in analyzing text by breaking it into meaningful parts, making it easier to process and interpret. It is an essential preprocessing step before transforming text into numerical representations for machine learning models. By understanding both individual words and their overall context, we can improve tasks such as **text classification, machine translation, and sentiment analysis**.  \n",
    "\n",
    "Now, let‚Äôs explore some examples of sentence and word tokenization using the `nltk` package.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee8b19-66ca-4581-817d-3ce095c56c5d",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f1efe23-c177-4107-bd39-95eb10b0587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527e98f-356a-46c8-91b0-a3d4bbd9375e",
   "metadata": {},
   "source": [
    "## Sentence tokenization :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac86c9-6fe9-4902-8a88-7485f04512ec",
   "metadata": {},
   "source": [
    "Sentence tokenization, also called sentence segmentation, is the process of splitting a text into individual sentences. This is useful in sentiment analysis when analyzing opinions sentence by sentence rather than as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f1dff-baea-49d1-bf0e-c09865aaa33e",
   "metadata": {},
   "source": [
    "#### How does it work ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275cebae-c48a-49ae-b81f-cf66592e7bd6",
   "metadata": {},
   "source": [
    "Most sentence tokenizers use regular expressions or pre-trained models to identify sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e865085d-33d5-4d5b-a3ba-2574458d3a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The new phone is amazing!', 'The battery lasts all day.', 'However, the camera quality could be better.', 'Overall, I love it!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize  \n",
    "\n",
    "text = \"The new phone is amazing! The battery lasts all day. However, the camera quality could be better. Overall, I love it!\"  \n",
    "sentences = sent_tokenize(text)  \n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1a87ed-9122-4c68-b6fe-fcaa09529ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr. Smith is a great doctor.', \"He works at St. John's Hospital!\", 'Do you know him?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Dr. Smith is a great doctor. He works at St. John's Hospital! Do you know him?\"  \n",
    "sentences = sent_tokenize(text)  \n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24c0d9-e021-4ba7-a6ce-6bfd21129fb7",
   "metadata": {},
   "source": [
    "## Word tokenization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be4958f1-0113-4bd1-a043-c74d8d2e8f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'absolutely', 'love', 'this', 'product', '!', 'It', \"'s\", 'fantastic', 'and', 'exceeded', 'my', 'expectations', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "\n",
    "text = \"I absolutely love this product! It's fantastic and exceeded my expectations.\"  \n",
    "tokens = word_tokenize(text)  \n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabe8e0-cd46-4e7b-9443-1dbdddbac3df",
   "metadata": {},
   "source": [
    "### Exemple : Handling Contractions Before Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f731f658-2103-4137-892b-8c2911d2d4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ca', \"n't\", 'believe', 'this', '!', 'It', \"'s\", 'horrible', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I can't believe this! It's horrible.\"  \n",
    "tokens = word_tokenize(text)  \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "133f08eb-415f-4496-800e-147bbfae3561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'can', 'not', 'believe', 'this', '!', 'It', \"'s\", 'horrible', '.']\n"
     ]
    }
   ],
   "source": [
    "import re  \n",
    "\n",
    "text = \"I can't believe this! It's horrible.\"  \n",
    "text_cleaned = re.sub(r\"can\\'t\", \"can not\", text)  # Expand contractions  \n",
    "text_cleaned = re.sub(r\"it\\'s\", \"it is\", text_cleaned)  \n",
    "\n",
    "tokens = word_tokenize(text_cleaned)  \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56474fe5-c661-471e-8311-1a6548424842",
   "metadata": {},
   "source": [
    "### Exemple : Tokenization with Special Characters & Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7014846-580d-4a9b-93bd-b7e5c9e0c9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'this', 'product', '!', '!', '!', 'üòç', 'It', \"'s\", 'the', 'best', 'ever', '!', '!', '#', 'HappyCustomer']\n"
     ]
    }
   ],
   "source": [
    "text = \"I love this product!!! üòç It's the best ever!! #HappyCustomer\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c1c68-efb1-4302-a580-d6d8465b8f17",
   "metadata": {},
   "source": [
    "### Example: Case Sensitivity in Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "925bbc10-30a9-4557-94d6-8bf0a641f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Her', 'service', 'was', 'outstanding', '!', 'her', 'kindness', 'made', 'my', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "\n",
    "text = \"Her service was outstanding! her kindness made my day.\"  \n",
    "tokens = word_tokenize(text)  \n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390fc2fa-b9bf-4f49-8734-3ca03e8d3325",
   "metadata": {},
   "source": [
    "\"Her\" and \"her\" are treated as different tokens.\n",
    "\n",
    "In sentiment analysis, this can lead to inconsistencies in frequency-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a2b0a-c2d9-4c85-ac65-6624f8f5b9c1",
   "metadata": {},
   "source": [
    "##### Solution: Convert Text to Lowercase Before Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c5c8b86-cf14-48be-840d-a3ae85c1d0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['her', 'service', 'was', 'outstanding', '!', 'her', 'kindness', 'made', 'my', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_lower = [word.lower() for word in tokens]  \n",
    "print(tokens_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98c840-6bff-4c9a-aa1c-846f72def30e",
   "metadata": {},
   "source": [
    "### Exemple : word tokenizing for a list of sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e2012b5-03c5-44d8-9eb2-c73c02981864",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"The product is amazing and works perfectly!\",\n",
    "    \"I really loved the experience of using this.\",\n",
    "    \"This is the worst purchase I have ever made.\",\n",
    "    \"The shipping was delayed, but the product is great.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16bb2a05-bae9-4a16-af12-8644d850849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = []\n",
    "for sentence in reviews :\n",
    "    token.append(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d768987a-82ed-4c20-b4fa-17c2a0faf453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'product', 'is', 'amazing', 'and', 'works', 'perfectly', '!'],\n",
       " ['I', 'really', 'loved', 'the', 'experience', 'of', 'using', 'this', '.'],\n",
       " ['This', 'is', 'the', 'worst', 'purchase', 'I', 'have', 'ever', 'made', '.'],\n",
       " ['The',\n",
       "  'shipping',\n",
       "  'was',\n",
       "  'delayed',\n",
       "  ',',\n",
       "  'but',\n",
       "  'the',\n",
       "  'product',\n",
       "  'is',\n",
       "  'great',\n",
       "  '.']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf2953-3b32-41b7-8831-dfd64d2d5f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and stem words in reviews\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
